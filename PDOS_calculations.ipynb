{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c821f7-f076-4da3-af6d-8e8c943564cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "import regex as re\n",
    "import os\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f6f66-baa5-4079-bded-984391ac6e20",
   "metadata": {},
   "source": [
    "# Hamiltonian of few layer graphene\n",
    "\n",
    "The reasons to why we build the Hamiltonian the way we do, can be found in my MSc Thesis. This section is ment to introduce the functions that create the momentum-space Hamiltonian. The calculations were done on a remote computer using the PDOS_calc.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29737d-fe6e-42ca-9e2a-60b5215e977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hk(D,t0,t1,t3,t4,N,in_list,k):\n",
    "    '''\n",
    "    This function creates a Hamiltonian for each k point sampled in the Brillouin zone.\n",
    "    Parameters: \n",
    "        -D: on-site energy\n",
    "        -t0,t1,t3,t4: hopping energies (notation weirdness comes from literature, where t2 is interaction of non-adjecent layers)\n",
    "        -N: number of layers\n",
    "        -in_list: list of faults, eg.:['1b','3b']\n",
    "        -k: k-points with shape k[0] = x-coordinate k[1] = y-coordinate (output of grid_maker function)\n",
    "    '''\n",
    "    kx=k[0]\n",
    "    ky=k[1]\n",
    "    F = 1 + np.exp(1j/2*(np.sqrt(3)*kx+ky)) + np.exp(1j/2*(np.sqrt(3)*kx-ky)) #First phase factor\n",
    "    F_A1B2 = np.exp(1j*ky*3) + np.exp(1j/2*(np.sqrt(3)*kx+ky)) + np.exp(1j/2*(np.sqrt(3)*kx-ky)) #Second phase factor\n",
    "    \n",
    "    #All sub-matrices are created first\n",
    "    h0_ABC = np.array([[0*np.ones(np.shape(F)),t0*F], #Intra-layer  \n",
    "                       [np.conj(t0*F),0*np.ones(np.shape(F))]]).transpose((2,0,1))\n",
    "    \n",
    "    D_mat = np.array([[0*np.ones(np.shape(F)),0*np.ones(np.shape(F))], #On-site\n",
    "                      [0*np.ones(np.shape(F)),D*np.ones(np.shape(F))]]).transpose((2,0,1))\n",
    "    \n",
    "    D_mat_b = np.array([[D*np.ones(np.shape(F)),0*np.ones(np.shape(F))], #On-site in case of stacking fault\n",
    "                      [0*np.ones(np.shape(F)),0*np.ones(np.shape(F))]]).transpose((2,0,1))\n",
    "    \n",
    "    h1 = np.array([[t4*F,t3*F_A1B2], #Inter-layer\n",
    "                   [t1*np.ones(np.shape(F)),t4*F]]).transpose((2,0,1))\n",
    "    \n",
    "    h1_b = np.conj(np.array([[t4*F,t1*np.ones(np.shape(F))], #Inter-layer in case of stacking fault\n",
    "                             [t3*F_A1B2,t4*F]])).transpose((2,0,1))\n",
    "    \n",
    "    \n",
    "    H0k = np.zeros((len(F),2*N,2*N),dtype=complex) #The whole matrix, first created with zeros in the right shape\n",
    "    \n",
    "    onsite_mat = np.kron(np.eye(N),h0_ABC) #making it the right size\n",
    "    \n",
    "    #This part is handling stacking faults\n",
    "    on_site_list = np.ones(N)\n",
    "    on_site_list[-1] = 0\n",
    "    on_site_list_b     = np.ones(N)\n",
    "    on_site_list_b[0] = 0\n",
    "    hoppingok_list = np.ones(N-1) \n",
    "    flag_T = False\n",
    "    print(in_list)\n",
    "    if (in_list == ['']): #This is just an ugly bug-fix\n",
    "        in_list = []\n",
    "    for string in in_list:\n",
    "        helyzet = (int(re.search(r'\\d+', string).group())) #Interpretation of input_list \n",
    "        hoppingok_list[helyzet-1]   -= 1\n",
    "        on_site_list[helyzet-1]   -= 1\n",
    "        on_site_list[helyzet]     += 1\n",
    "        on_site_list_b[helyzet-1] += 1\n",
    "        on_site_list_b[helyzet]   -= 1\n",
    "        #This is to create vectors which have ones in the correct places, so we can make a kronecher product \n",
    "    \n",
    "    hopping_mat = np.kron(np.diag(hoppingok_list,k=1),h1) + np.kron(np.diag(1-hoppingok_list,k=1),h1_b)\n",
    "    hopping_mat += np.conj(np.transpose(hopping_mat,[0,2,1]))\n",
    "    onsite_mat += np.kron(np.diag(on_site_list),D_mat) + np.kron(np.diag(on_site_list_b),D_mat_b)\n",
    "    #To prove that this is the correct way of building the matrices is left as an excercise for the reader \n",
    "    \n",
    "    H0k += onsite_mat\n",
    "    H0k += hopping_mat #Adding up the matrix\n",
    "    return H0k\n",
    "\n",
    "def grid_maker(vec1,vec2,num,misplace=[0,0]):\n",
    "    '''\n",
    "    This function creates a grid of the paralelogramm created of two vectors\n",
    "    '''\n",
    "    points = []\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            points.append([misplace[0] + i*vec1[0]/num + j*vec2[0]/num,misplace[1] + i*vec1[1]/num + j*vec2[1]/num])\n",
    "    return np.array(points).reshape(num*num,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf77e4-0a91-4652-838d-6308748aabde",
   "metadata": {},
   "source": [
    "# Computing all possible stacking configurations up to N layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d69be2-1be7-4197-96c7-84df7351864c",
   "metadata": {},
   "source": [
    "We rule out AA stacking since it is energetically less favourable. After i number of layers the i+1th can be hence put either to the left or the right of the previous one. Let us denote right +1 and left -1, which enables us to encode each configuration with a string of +1s and -1s.\n",
    "$$\\mathrm{ABA} \\Rightarrow -1+1 = +1-1$$\n",
    "$$\\mathrm{ABC} \\Rightarrow +1+1 = -1-1$$\n",
    "Configurations that are topologically not different will result in the same PDOS. Finding all topologically different configurations is an important task before the calculations since it decreases computation time significantly.\n",
    "Topologically different means in this case that the strings can not be transformed into each other with flipping the signs or reversing the order or both. (In fact, these operations are equivalent with mirroring and rotating the geometry itself.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de350d6-5d12-4812-94e4-e5d7eb9e4020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combination_list(N):\n",
    "    '''\n",
    "    Create a list containing all topologically different combinations of each layer up to N in the format required for in_list.\n",
    "    For example the 5th element of the output list has all possible configurations for N=5 layers.\n",
    "    '''\n",
    "    ossz_kombok_unzipped = [] #store combination lists\n",
    "    for N in range(0,11):\n",
    "        faults = np.array([f'{i}b' for i in range(1,N)])\n",
    "        perms = []\n",
    "        kombok = []\n",
    "        #These cycles check the conditions mentioned above\n",
    "        for i in range(int(np.ceil(N/2))):\n",
    "            for perm in sorted(set(permutations(np.concatenate((-1*np.ones(i),np.ones(N-1-i)))))):\n",
    "                if list(perm)[::-1] not in perms and list(-1*np.array(perm)) not in perms:\n",
    "                    perms.append(list(perm))\n",
    "                    kombok.append((faults[np.array(perm) == -1]).tolist())\n",
    "        ossz_kombok_unzipped.append(kombok)    \n",
    "    return ossz_kombok_unzipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61e129-958d-47bd-8fe6-ce98f87e8edb",
   "metadata": {},
   "source": [
    "# Computing projected density of states (PDOS)\n",
    "\n",
    "The PDOS is the DOS weighted with the absolute value squared of the eigenvectors. In the numerical calculations the Dirac delta in the definition of DOS can be substituted with a narrow Gaussian:\n",
    "$$\\mathrm{PDOS}_n = N \\sum_i^{\\mathrm{bands}}\\sum_{j=1}^N |\\Psi_n|^2\\frac{1}{\\alpha\\sqrt{\\pi}}e^{-\\frac{(E-E_i)^2}{\\alpha^2}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ae7a0-5221-4d78-9972-7dee9e78f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PDOS(E,Energies,w,alpha,norm):\n",
    "    '''\n",
    "    Function to calculate the PDOS in the energy range E from the eigenvalues and eigenvectors of the Hamiltonian created.\n",
    "    Parameters:\n",
    "        -E: energy range\n",
    "        -Energies: eigenvalues\n",
    "        -w: eigenvectors\n",
    "        -alpha: smoothing factor\n",
    "        -norm: could be calculated, but left as 1 in the work done for the thesis\n",
    "    '''\n",
    "    Energies = Energies.flatten()\n",
    "    w = np.transpose(w, axes=[0,2,1]) #this is to have the vectors in the correct shape. If not gotten from np.eigh it might be buggy\n",
    "    w = abs(w.reshape(w.shape[0]*w.shape[1], w.shape[2]))**2\n",
    "    D = norm/alpha * w.T @ np.exp(-((Energies[:,None] - E)**2)/(2*alpha**2))\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a983a4b-7b3d-4a48-ba2d-666bb6918638",
   "metadata": {},
   "source": [
    "# Evaluation with tree model\n",
    "\n",
    "This part showcases a very rudimental application of the decision tree algorithm offered by sklearn. After the PDOS spectra are obtained it proved to be very complicated to check 200+ stacking configurations by hand and identify the substructures in them, thus a decisiontree was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b464f2-0c75-4136-a63f-f201f245498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This part is very specific to my dataset. If it were to be recreated than these steps could be omitted\n",
    "'''\n",
    "directories_old = [name for name in os.listdir(\"sergej_gamma_1001_E_newcombs//sergej_gamma_1001_E//\") if os.path.isdir(f'sergej_gamma_1001_E_newcombs//sergej_gamma_1001_E//{name}')]\n",
    "pdos_list_old = [genfromtxt(f'sergej_gamma_1001_E_newcombs//sergej_gamma_1001_E//{s}//pdos_normed.dat',delimiter=';') for s in directories_old]\n",
    "directories_new = [name for name in os.listdir(\"getdicts//\") if os.path.isdir(f'getdicts//{name}')]\n",
    "directories = [directory for directory in directories_old if directory in directories_new]\n",
    "pdos_list = [pdos for i,pdos in enumerate(pdos_list_old) if directories_old[i] in directories_new]\n",
    "'''\n",
    "This is the poin where the reading in of the dataset is done.\n",
    "'''\n",
    "E = linspace(-1,1,1001) #the energy range of the calculations\n",
    "faultok_list = [s.split('[', 1)[1].split(']')[0].split(', ') for s in directories] #which faults were used\n",
    "N_list = [int(s.split('_')[0]) for s in directories] \n",
    "\n",
    "'''\n",
    "We only use the sum of the top and bottom layers' sublattice points\n",
    "'''\n",
    "pdos_top_sum_list = [pdos[-1] + pdos[-2] for pdos in pdos_list]\n",
    "pdos_bot_sum_list = [pdos[0] + pdos[1] for pdos in pdos_list]\n",
    "\n",
    "'''\n",
    "Only the middle of the spectrum is important\n",
    "'''\n",
    "pdos_top_sum_cut_list = [pdos_top_sum[len(pdos_top_sum)//4:3*len(pdos_top_sum)//4] for pdos_top_sum in pdos_top_sum_list]\n",
    "pdos_bot_sum_cut_list = [pdos_bot_sum[len(pdos_bot_sum)//4:3*len(pdos_bot_sum)//4] for pdos_bot_sum in pdos_bot_sum_list]\n",
    "\n",
    "'''\n",
    "Preparing the dataset to be used\n",
    "'''\n",
    "pdos_top_sum_df = pd.DataFrame(data=pdos_top_sum_cut_list,columns=E[len(E)//4:3*len(E)//4])\n",
    "pdos_bot_sum_df = pd.DataFrame(data=pdos_bot_sum_cut_list,columns=E[len(E)//4:3*len(E)//4]) #pandas dataframes can be easily handled by sklearn\n",
    "pdos_top_sum_df_train = pd.concat((pdos_top_sum_df[136:],pdos_bot_sum_df[136:]),ignore_index=True) #train dataset, omitting 10 layers\n",
    "pdos_top_bot_sum_df_train = pdos_top_sum_df_train.round(5).drop_duplicates() #dropping duplicates, this helps the learning algorithm\n",
    "\n",
    "y_train_top = ['TOP_' + directory for directory in directories[136:]]\n",
    "y_train_bot = ['BOT_' + directory for directory in directories[136:]]\n",
    "y_train = y_train_top + y_train_bot \n",
    "\n",
    "y_test_top = ['TOP_' + directory for directory in directories[:136]]\n",
    "y_test_bot = ['BOT_' + directory for directory in directories[:136]]\n",
    "y_test = y_test_top + y_test_bot #Giving proper class names\n",
    "\n",
    "y_train = array(y_train)[pdos_top_bot_sum_df_train.index] #this solves some issues I guess\n",
    "\n",
    "'''\n",
    "The training\n",
    "'''\n",
    "clf = DecisionTreeClassifier(random_state=69420).fit(pdos_top_bot_sum_df_train,y_train) #the built-in decision tree\n",
    "\n",
    "'''\n",
    "Prediction dataset\n",
    "'''\n",
    "pdos_test = pd.concat((pdos_top_sum_df[:136],pdos_bot_sum_df[:136]),ignore_index=True).round(5)\n",
    "directories_pred_abs = clf.predict(pdos_test)\n",
    "\n",
    "'''\n",
    "The evaluation was done by hand.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
